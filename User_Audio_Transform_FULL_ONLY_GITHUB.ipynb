{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, datetime\n",
    "\n",
    "from pyAudioAnalysis import audioBasicIO, ShortTermFeatures\n",
    "from pydub import AudioSegment\n",
    "\n",
    "import soundfile\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is used to convert the user's recorded audio (vocal-only) and transform it using a vocal profile of one of the speakers in the datasets. The full conversion is not expected to be a complete vocal replacement but more of a shift towards the chosen speaker's voice.\n",
    "\n",
    "The first part of this script will be very similar to Audio_Properties_Extract to extract the properties per sample in the audio recording. (Sample = one sample in a sample rate of a audio file)\n",
    "Conversions will be stored in the ~\\Vocal_Replace\\User_Vocal_Data\\User_Transformation folder.\n",
    "\n",
    "The second part is to use the audio extraction and the vocal profile of the chosen speaker as the variables to feed into the model, which the model will attempt to predict the amplitude of each sample. Most of the coding will be from the Deep_Learning_Two_To_One_Model script.\n",
    "Final prediction audio will be placed in ~\\Vocal_Replace\\Model\\Model_Outputs folder. There will be two predictions done: one using a model trained on all speakers and another prediction using a model trained only with male or female speakers (depending on the gender of the chosen speaker).\n",
    "\n",
    "Manual file moving and variable declaration is required before using this script:\n",
    "\n",
    "1) Move the user's audio file (in WAV file format and ONLY IN MONO not stereo) into the ~\\Vocal_Replace\\User_Vocal_Data\\Recordings folder\n",
    "    If the user's audio is not in WAV and/or is not a mono channel recording, it is better to use Audacity to manually convert the audio over. (Although there are Python packages that will cover some of the conversions needed, they have limited capabilities on reading different audio files. Audacity is free and widely used, and has a lot of support for almost anything audio-related.)\n",
    "\n",
    "2) Rename the file to with a unique filename\n",
    "\n",
    "3) Take that filename (with the file extension) and set it to be the value for the variable, user_audio_filename\n",
    "    user_audio_filename = \"audio file.wav\"\n",
    "    \n",
    "4) Choose a speaker/vocal profile for the vocal replacement and set their ID number as the value for the speaker_ID variable. \n",
    "    To see an overview of speakers, look at the SPEAKER_Modified_Final.csv file. To see the vocal profiles, look in the Vocal Profile Summary.csv file. Do note that not all of the speakers in the SPEAKER_Modified_Final.csv have vocal profiles--that is because not all the datasets avaliable were used in this project and it is better to overview the avaliable speakers using the Vocal Profile Summary.csv instead. If the datasets were downloaded and unzipped, you can listen to the speaker's voice by playing the audio file in that dataset. The dataset that the speaker belongs to is listed in the SPEAKER_modified_final.csv file.\n",
    "    \n",
    "Note: Once audio recording will take up a lot of storage, especially RAM! A ~8 minute recording will take up 10-12 GBs of storage and RAM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### MANUAL INPUT IN THIS CELL ###################################################################\n",
    "\n",
    "# Declare the filename of the user's vocal audio file:\n",
    "user_audio_filename = \"Fire and Ice - Robert Frost - Recording.wav\"\n",
    "speaker_ID = 61\n",
    "\n",
    "#############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the filepaths to the user's audio file:\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "user_audio_filepath = current_directory + '\\\\Vocal_Replace\\\\User_Vocal_Data\\\\Recordings\\\\' + user_audio_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio Extraction:\n",
    "\n",
    "###### Loading in converted audio file:\n",
    "sample_rate, audio_amplitude = audioBasicIO.read_audio_file(user_audio_filepath)\n",
    "\n",
    "###### \"window\" = 512, \"step\" = 1\n",
    "feature_values, feature_names = ShortTermFeatures.feature_extraction(audio_amplitude, sample_rate, window= 512, step= 1)\n",
    "\n",
    "###### Creating a DF of the features extracted in the previous line of code:\n",
    "###### The features and their definitions are found here (https://github.com/tyiannak/pyAudioAnalysis/wiki/3.-Feature-Extraction)\n",
    "feature_df = pd.DataFrame()\n",
    "\n",
    "for df_index in range(len(feature_names)):\n",
    "    feature_df[feature_names[df_index]] = feature_values[df_index]\n",
    "\n",
    "feature_df['SAMPLE_RATE'] = sample_rate\n",
    "\n",
    "feature_df.to_csv(current_directory + '\\\\Vocal_Replace\\\\User_Vocal_Data\\\\User_Transformation\\\\' + user_audio_filename \n",
    "                  + ' - Audio Extraction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the Vocal Profiles, converting genders, and selecting only the chosen speaker according to the input speaker_ID:\n",
    "vocal_profile_df = pd.read_csv(current_directory \n",
    "                               + '\\\\Vocal_Replace\\\\Data\\\\Modified_Data\\\\Vocal_Profiles\\\\Vocal Profile Summary.csv', \n",
    "                               index_col=0) \n",
    "\n",
    "# Converting gender into numeric (0, 1):\n",
    "for index, row in vocal_profile_df.iterrows():\n",
    "    if row.GENDER == 'M':\n",
    "        gender = 1\n",
    "        \n",
    "    else:\n",
    "        gender = 0\n",
    "        \n",
    "    vocal_profile_df.loc[index, 'GENDER'] = gender\n",
    "    \n",
    "vocal_profile_df = vocal_profile_df.astype({'GENDER': np.int64})\n",
    "\n",
    "vocal_profile_selected = vocal_profile_df[vocal_profile_df.SPEAKER_ID == speaker_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in trained models and predicting a audio file:\n",
    "\n",
    "# Reloading in the feature_df file:\n",
    "feature_df = pd.read_csv(current_directory + '\\\\Vocal_Replace\\\\User_Vocal_Data\\\\User_Transformation\\\\' + user_audio_filename \n",
    "                  + ' - Audio Extraction.csv', index_col= 0)\n",
    "\n",
    "# Loading in the trained models and definitions of custom metrics used:\n",
    "def custom_metric_mean(y_true, y_pred):\n",
    "    ## Want this to be as close to 0 as possible\n",
    "    return keras.backend.mean(y_true, axis=None, keepdims=False) - keras.backend.mean(y_pred, axis=None, keepdims=False)\n",
    "\n",
    "# Defining custom metric:\n",
    "def custom_metric_difference(y_true, y_pred):\n",
    "    ## Want this to be as close to 0 as possible\n",
    "    return keras.backend.sum(y_true, axis=None, keepdims=False) - keras.backend.sum(y_pred, axis=None, keepdims=False)\n",
    "\n",
    "# Loading in the three pre-trained models (male + female, male only, female only):\n",
    "model_full_loaded = tf.keras.models.load_model(current_directory \n",
    "                                               + '\\\\Vocal_Replace\\\\Model\\\\Saved_Models\\\\Saved_Model_Full.h5', \n",
    "                                               custom_objects={\"custom_metric_mean\": custom_metric_mean, \n",
    "                                                               'custom_metric_difference': custom_metric_difference,\n",
    "                                                              'PReLU': keras.layers.PReLU()}, \n",
    "                                               compile=False)\n",
    "model_full_loaded.compile(optimizer= keras.optimizers.Adam(learning_rate=0.001), loss= 'mean_squared_error', \n",
    "                          metrics= [custom_metric_mean, custom_metric_difference])\n",
    "\n",
    "\n",
    "\n",
    "# Extending the vocal_profile selected to be the same length as feature_df:\n",
    "extended_vocal_profile = pd.DataFrame({'SPEAKER_ID': vocal_profile_selected.SPEAKER_ID.values[0], \n",
    "                                       'GENDER': vocal_profile_selected.GENDER.values[0], \n",
    "                                       'MIN_RANGE': vocal_profile_selected.MIN_RANGE.values[0], \n",
    "                                       'MAX_RANGE': vocal_profile_selected.MAX_RANGE.values[0], \n",
    "                                       'AVERAGE_RANGE': vocal_profile_selected.AVERAGE_RANGE.values[0],\n",
    "                                       'STD_RANGE': vocal_profile_selected.STD_RANGE.values[0], \n",
    "                                       'MEDIAN_RANGE': vocal_profile_selected.MEDIAN_RANGE.values[0]},\n",
    "                                     index=range(len(feature_df)))\n",
    "\n",
    "# Converting feature_df & vocal_profile_selected to numpy array to be used in prediction:\n",
    "audio_input = feature_df.values\n",
    "extended_vocal_profile = extended_vocal_profile.values\n",
    "\n",
    "# Predicting & converting the predictions into a WAV file:\n",
    "model_full_prediction = model_full_loaded.predict([audio_input, extended_vocal_profile])\n",
    "soundfile.write(file= current_directory + '\\\\Vocal_Replace\\\\Model\\\\Model_Outputs\\\\Full Model - Transformed With ' \n",
    "                    + str(speaker_ID) + ' - ' + user_audio_filename, \n",
    "                data= model_full_prediction, samplerate= feature_df.SAMPLE_RATE[0])\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
